model='tf_lstm'

# Large model (Zaremba 2014) with valiational dropout
# same hyperparameter of `A Theoretically Grounded Application of Dropout in Recurrent Neural Networks`
# leaning rate: 1
# decay after 14 eoich: divide by 1.15
# epoch 55

learning_rate=1.0
max_epoch=14
learning_rate_decay=1.15

gradient_clip=10
batch_size=20
#layer_norm=false
keep_prob=[0.5, 0.5]  # input, output
keep_prob_r=[0.5]
weight_decay=1e-7
optimizer='sgd'
ini_scale=0.04

[config]
n_lstm_layer=2
vocab_size=10000
num_steps=35
embedding_size=1500
num_units=1500
recurrent_dropout=false
forget_bias = 0.0